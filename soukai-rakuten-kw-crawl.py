#!/usr/bin/python# -*- coding: utf-8 -*-from bs4 import BeautifulSoupfrom common_util import *get_cron_info(60);import jsonimport reimport requestsimport timeimport datetimeimport logging# import sys# reload(sys)# sys.setdefaultencoding('utf-8')from pyvirtualdisplay import Displayfrom selenium import webdriverfrom selenium.common.exceptions import NoSuchElementExceptionfrom selenium.webdriver.chrome.options import Options options = Options()#options.add_argument("--headless")options.add_argument('--no-sandbox')#options.add_argument('--disable-gpu')options.add_argument('start-maximized')options.add_argument('disable-infobars')#options.add_argument("--disable-extensions")options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36")driver = webdriver.Chrome(options=options, executable_path=r'C:\\driver\\chromedriver.exe')  '''display = Display(visible=0, size=(800, 600))display.start()chrome_options = webdriver.ChromeOptions()chrome_options.add_argument('--no-sandbox')driver = webdriver.Chrome('/usr/local/bin/chromedriver', chrome_options=chrome_options)'''# driver =webdriver.Chrome()#connecion stringconnection = get_jp_connection()#connecion string	ts = time.time()date = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d')timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')#logging.basicConfig(filename='log/singapore-lazada-kw-scrap'+date+'.log',level=logging.DEBUG)#logging.info('Crawling starts at:'+timestamp)#logging.debug('This message should go to the log file')#logging.warning('And this, too')#errorLog = open('amazon-pdp-error_log.txt', 'w')keywords = []brands = []locations = []index = 0keyword_count = 0vpf_id = '3' 			# soukai rakutenvcrawl_id = '0'			## MASTERvkeyword = '0'			## MASTERvkeyword_id = '0'		## MASTERvbrand_id = '0'			## MASTERvbrand_name = '0'		## MASTERvlocation_id = '0'		## MASTERvlocation_name = '0'	        ## MASTERvpincode = '0'			## MASTERvcreated_by= 'System'	        #$ DEFAULTdef crawl_datacapture(keywordArr):    vkeyword = keywordArr['keyword']		## MASTER    vkeyword_id = keywordArr['kw_id']    vbrand_name = keywordArr['brand_name']	        ## MASTER    vbrand_name_th = keywordArr['brand_name_th']	## MASTER    vbrand_id = keywordArr['brand_id']		## MASTER        url = 'https://search.rakuten.co.jp/search/mall/'+vkeyword+'/?sid=203677'    driver.get(url)    time.sleep(5)    html_doc = None    soup = None    productList1= None    productList=[]    try:        html_doc = driver.find_element_by_class_name("dui-cards").get_attribute('innerHTML')    except NoSuchElementException:        time.sleep(2)            if html_doc is not None:        soup = BeautifulSoup(html_doc, 'html.parser')        productList = soup.findAll("div", {"class": "dui-card"}, recursive=True)            if len(productList)==0:        #INSERT QUERY FOR NO PRODUCT FOUND        print('no products found response')        sql = "INSERT INTO `soukai_rakuten_crawl_kw` (`pf_id`,`crawl_id`, `keyword`, `keyword_id`,`brand_id`,`brand_name`,`brand_name_th`, `brand_crawl`, `url_code_crawl`, `pdp_title_value`,`position`, `price_sp`,`price_rp`, `pdp_rating_value`,`pdp_rating_count`, `pdp_image_url`,`pdp_page_url`,`pdp_discount_value`, `location_id`,`location_name`,`pincode`,`created_by`,`web_pid`,`reseller_name_crawl`,`is_rb` )VALUES (%s,%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,%s,%s,%s)"        try:            cursor.execute(sql, (            vpf_id            , vcrawl_id            , vkeyword            , vkeyword_id            , vbrand_id            , vbrand_name            , vbrand_name_th.strip()[:65535]            , '0'            , '0'            , '0'            , '0'            , '0'            , '0'            , '0'            , '0'            , '0'            , '0'            , '0'            , vlocation_id            , vlocation_name            , vpincode            , vcreated_by            , '0'            , '0'            , 0            ))            connection.commit()        except cursor.Error as e:            e=str(e)            print(e)            # creating error file. it will be replaced with send email function            # sendmail(e)            # errorLog.write(e)        return '1'    vposition = '0'    index = 0    #get all brands     cursor.execute("SELECT brand_name,is_rb, brand_name_th FROM `rb_brands`")    allBrands= cursor.fetchall()    for product in productList:        vpdp_title_value = '0'        vbrand_crawl = '0'        vweb_pid = '0'        vprice_sp = 0        vpdp_rating_value = 0        vpdp_rating_count = 0        vpdp_image_url = '0'        vpdp_page_url = '0'        vpdp_discount_value = 0        vurl_code_crawl = '0'        is_rb =0        ## START :: TITLE BLOCK  ##        title_div = product.find("h2")        if title_div is None:            continue                    ttl= title_div.find('a')                if ttl is not None:            title = ttl.text            vpdp_title_value = title.strip()            linkurl = ttl["href"]            vpdp_page_url = linkurl            index = index+1                if index>20:            break        ## START :: BRAND CRAWL BLOCK  ##        for brand  in allBrands:            if brand['brand_name'] in vpdp_title_value:                vbrand_crawl = brand['brand_name']                is_rb = brand['is_rb']                continue                            if brand['brand_name_th'] in vpdp_title_value:                vbrand_crawl = brand['brand_name_th']                is_rb = brand['is_rb']                continue        if vbrand_crawl == '0':            vbrand_crawl = 'Others'        ## END :: BRAND CRAWL BLOCK  ##                ## START :: POSITION CRAWL BLOCK  ##        vposition = index        ## END :: POSITION CRAWL BLOCK  ##        ## START :: PIRCE BLOCK  ## c-product-card__price-final        price_div= product.find('div',{'class':'content description price'})        if price_div is not None:            price =price_div.findAll('span')[0].text.replace(',','').strip()            vprice_sp =float(''.join(filter(str.isdigit, price)))            vprice_rp = vprice_sp        else:            vprice_sp=0            vprice_rp=0                discount = float(vprice_rp) - float(vprice_sp)        if discount >0:            vpdp_discount_value = round((discount/float(vprice_rp))*100,2)        else:            vpdp_discount_value = 0                        ## END :: PIRCE BLOCK  ##        pdp_rating_value ='0'        vpdp_rating_count = '0'        review_block= product.find('div',{'class':'content review'})        if review_block is not None:            pdp_rating_count= review_block.find('span', {'class':'legend'}).text.strip()            pdp_rating_count= ''.join(filter(str.isdigit, pdp_rating_count))            pdp_rating_value= review_block.find('span', {'class':'score'}).text.strip()                           ## END :: RATING BLOCK  ##                        ## START :: IMAGE BLOCK  ##                 imageDiv = product.find("img")        if imageDiv is not None:            vpdp_image_url = imageDiv["src"]        else:            vpdp_image_url='0'        ## END :: IMAGE BLOCK  ##        # START :: WEB PID CRAWL BLOCK  ##        vurl_code_crawl = product['data-duiid']        vreseller_name_crawl ='0'        vpdp_rating_count = 0        vpdp_rating_value = 0                        ## END :: WEB PID CRAWL BLOCK  ##        vweb_pid=vurl_code_crawl        ## START :: PAGE URL BLOCK  ##        ## START :: INSERT QUERY BLOCK  ##        print('pf_id::'+str(vpf_id))        print('crawl_id::'+str(vcrawl_id))        print('keyword::'+str(vkeyword))        print('keyword_id::'+str(vkeyword_id))        print('brand_id::'+str(vbrand_id))        print('brand_name::'+str(vbrand_name))        #print('brand_name_th::'+str(vbrand_name_th))        print('brand_crawl::'+str(vbrand_crawl.strip()[:65535]))        print('web_pid::'+str(vweb_pid))        print('pdp_title_value::'+str(vpdp_title_value.strip()[:65535]))        print('position::'+str(vposition))        print('price_sp::'+str(vprice_sp))        print('price_rp::'+str(vprice_rp))        print('pdp_rating_value::'+str(vpdp_rating_value))        print('pdp_rating_count::'+str(vpdp_rating_count))        print('pdp_image_url::'+str(vpdp_image_url))        print('pdp_page_url::'+str(vpdp_page_url))        print('pdp_discount_value::'+str(vpdp_discount_value))        print('location_id::'+str(vlocation_id))        print('location_name::'+str(vlocation_name))        print('pincode::'+str(vpincode))        print('url_code_crawl::'+str(vurl_code_crawl))        print ('reseller_name_crawl ::'+str(vreseller_name_crawl.strip()[:65535]))        print('insert query')        sql = "INSERT INTO `soukai_rakuten_crawl_kw` (`pf_id`,`crawl_id`, `keyword`, `keyword_id`,`brand_id`,`brand_name`,`brand_name_th`, `brand_crawl`, `url_code_crawl`, `pdp_title_value`,`position`, `price_sp`,`price_rp`,`pdp_rating_value`, `pdp_rating_count`, `pdp_image_url`,`pdp_page_url`,`pdp_discount_value`, `location_id`,`location_name`,`pincode`,`created_by` ,`web_pid`,`reseller_name_crawl`,`is_rb`)VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,%s,%s,%s,%s)"        try:                cursor.execute(sql, (                vpf_id                , vcrawl_id                , vkeyword    #.encode("utf-8").strip()[:65535]                , vkeyword_id                , vbrand_id                , vbrand_name                , vbrand_name_th.strip()[:65535]                , vbrand_crawl.strip()[:65535]                , vurl_code_crawl.strip()[:65535]                , vpdp_title_value.strip()[:65535]                , vposition                , vprice_sp                , vprice_rp                , vpdp_rating_value                , vpdp_rating_count                , vpdp_image_url                , vpdp_page_url                , vpdp_discount_value                , vlocation_id                , vlocation_name                , vpincode                , vcreated_by                , vweb_pid                , vreseller_name_crawl                , is_rb                ))                connection.commit()        except cursor.Error as e:                e=str(e)                print(e)try:            with connection.cursor() as cursor:        sql1 = "SELECT k.kw_id,k.keyword, b.brand_id,b.brand_name,b.brand_name_th FROM `rb_keyword` AS k, rb_brands AS b WHERE k.brand_id=b.brand_id and k.status='1' AND k.keyword_type ='1'"                cursor.execute(sql1, ())        result2 = cursor.fetchall()        for keyword in result2:            keywords.append(keyword)        ts = time.time()        timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')        sql = "INSERT INTO `rb_crawl` (`pf_id`, `start_time`,`no_of_sku_parsed`, `crawl_type`) VALUES (%s, %s, %s, %s)"        try:            cursor.execute(sql, (vpf_id,timestamp,'0','2'))        except cursor.Error as e:            e=str(e)            # creating error file. it will be replaced with send email function            #errorLog.write(e)        vcrawl_id = connection.insert_id()        #vcrawl_id = 1        product_count = 0        for keywordArr in keywords:            val = crawl_datacapture(keywordArr)            keyword_count = keyword_count+1            driver.close()            driver = webdriver.Chrome(options=options, executable_path=r'C:\\driver\\chromedriver.exe')        keyword_count = str(keyword_count)        vcrawl_id = str(vcrawl_id)        ts = time.time()        timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')        try:            print('completed')            cursor.execute("UPDATE rb_crawl SET status=1, end_time = '"+timestamp+"',no_of_sku_parsed='"+keyword_count+"' WHERE crawl_id='"+vcrawl_id+"'")        except cursor.Error as e:            e=str(e)            print(e)            # creating error file. it will be replaced with send email function            # sendmail(e)            #errorLog.write(e)        try:            print('completed')            cursor.execute("UPDATE rb_platform SET  kw_crawl_data_date = '"+timestamp+"', kw_crawl_data_id='"+vcrawl_id+"' WHERE pf_id='"+vpf_id+"'")        except cursor.Error as e:            e=str(e)            print(e)        try:            kpi_id = '60'            kpi_name = 'JP_Saukai_kw'            run_status = 'success'            update_cron_status(kpi_id,kpi_name,run_status);        except Exception as e:            e=str(e)            # creating error file. it will be replaced with send email function            # sendmail(e)            #errorLog.write(e)                    try:            u_sql = "UPDATE soukai_rakuten_crawl_kw SET brand_crawl = 'Dr Scholl'  , is_rb ='1'  WHERE pdp_title_value LIKE '%Dr.Scholl%'"            cursor.execute(u_sql)        except cursor.Error as e:            e=str(e)            print(e)			except cursor.Error as e:	print(e)	# creating error file. it will be replaced with send email function	# sendmail(e)	#errorLog.write(e)finally:	driver.quit()	connection.commit()	connection.close()